import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.interpolate import CubicSpline, UnivariateSpline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from joblib import Parallel, delayed

def weighted_rescale_multivariate(x, y):
    """
    Rescale multivariate x and y using weighted normalization.
    """
    weights = np.linspace(0.1, 1.0, len(x))
    x_rescaled = (x - np.average(x, axis=0, weights=weights)) / np.ptp(x, axis=0)
    y_rescaled = (y - np.average(y, weights=weights)) / np.ptp(y)
    return x_rescaled, y_rescaled, weights

def compute_residuals_multivariate(y_true, y_pred):
    """
    Compute residuals for multivariate regression.
    """
    return y_true - y_pred

def find_best_polynomial_degree_multivariate(x_sampled, y_sampled, max_degree=5):
    """
    Find the best polynomial degree for multivariate regression error bounds.
    """
    best_degree = 1
    min_mse = float('inf')

    for degree in range(1, max_degree + 1):
        poly = PolynomialFeatures(degree)
        x_poly = poly.fit_transform(x_sampled)
        model = LinearRegression().fit(x_poly, y_sampled)
        y_pred = model.predict(x_poly)
        mse = mean_squared_error(y_sampled, y_pred)

        if mse < min_mse:
            min_mse = mse
            best_degree = degree

    return best_degree

def recursive_regression_with_adaptive_sampling_multivariate(
    x_train, y_train, max_iterations=10, sampling_factor=1.5, max_new_points=10
):
    """
    Recursive regression with adaptive sampling for multivariate input.
    """
    x_rescaled, y_rescaled, weights = weighted_rescale_multivariate(x_train, y_train)
    indices = np.random.choice(len(x_rescaled), 10, replace=False)
    x_sampled = x_rescaled[indices]
    y_sampled = y_rescaled[indices]

    for iteration in range(max_iterations):
        poly = PolynomialFeatures(degree=2)
        x_poly = poly.fit_transform(x_sampled)
        model = LinearRegression().fit(x_poly, y_sampled)
        y_pred = model.predict(poly.transform(x_rescaled))

        residuals = compute_residuals_multivariate(y_rescaled, y_pred)
        residual_threshold = sampling_factor * np.std(residuals)
        new_indices = np.where(np.abs(residuals) > residual_threshold)[0]

        if len(new_indices) > max_new_points:
            new_indices = np.random.choice(new_indices, max_new_points, replace=False)

        if len(new_indices) == 0:
            print(f"Converged at iteration {iteration + 1}.")
            break

        x_sampled = np.vstack([x_sampled, x_rescaled[new_indices]])
        y_sampled = np.hstack([y_sampled, y_rescaled[new_indices]])

        print(f"Iteration {iteration + 1}: Sampled points = {len(x_sampled)}, "
              f"Total points = {len(x_rescaled)}")

    best_degree = find_best_polynomial_degree_multivariate(x_sampled, y_sampled)

    # Fit the final model using the best degree
    poly = PolynomialFeatures(degree=best_degree)
    x_poly = poly.fit_transform(x_sampled)
    final_model = LinearRegression().fit(x_poly, y_sampled)

    def model_func(x):
        x_scaled = (x - np.average(x_train, axis=0, weights=weights)) / np.ptp(x_train, axis=0)
        x_poly = poly.transform(x_scaled)
        y_pred = final_model.predict(x_poly)
        return y_pred * np.ptp(y_train) + np.average(y_train, weights=weights)

    return model_func, best_degree

def generate_multivariate_datasets(option=1):
    """
    Generate multivariate datasets with up to 8 features based on the selected option.
    """
    np.random.seed(42)
    if option == 1:
        x = np.random.rand(500, 3) * 10
        y = 2 * x[:, 0]**2 - 3 * x[:, 1] + np.sin(x[:, 2]) + np.random.randn(500) * 0.5
    elif option == 2:
        x = np.random.rand(500, 5) * 10
        y = np.sum(3 * x[:, :3], axis=1) + np.sum(np.cos(x[:, 3:5]), axis=1) + np.random.randn(500) * 0.5
    elif option == 3:
        x = np.random.rand(500, 8) * 10
        y = (
            4 * x[:, 0] - 0.5 * x[:, 1]**2 + 2 * np.sqrt(x[:, 2]) +
            np.sum(np.log1p(np.abs(x[:, 3:6])), axis=1) +
            np.random.randn(500) * 0.5
        )
    elif option == 4:
        x = np.random.rand(500, 6) * 10
        y = (
            5 * np.sin(x[:, 0]) + x[:, 1]**3 - 2 * x[:, 2] +
            np.cos(x[:, 3]) * x[:, 4] - np.log1p(x[:, 5]) +
            np.random.randn(500) * 0.5
        )
    elif option == 5:
        x = np.random.rand(500, 4) * 10
        x[:, 3] = np.clip(x[:, 3], -np.pi / 2 + 0.1, np.pi / 2 - 0.1)  # Clip to avoid extreme values
        y = (
                np.sum(x[:, :2] ** 2, axis=1) - 3 * np.exp(-x[:, 2]) +
                np.tan(x[:, 3]) + np.random.randn(500) * 0.5
        )
    else:
        raise ValueError("Invalid dataset option.")

    return x, y

def plot_features_and_model(x, y, model_func=None):
    """
    Plot each feature against the dependent variable and overlay the model prediction.
    """
    feature_names = [f"Feature {i + 1}" for i in range(x.shape[1])]
    n_features = x.shape[1]

    plt.figure(figsize=(20, 10))

    for i in range(n_features):
        plt.subplot(2, (n_features + 1) // 2, i + 1)
        plt.scatter(x[:, i], y, alpha=0.7, label="Data Points")
        plt.xlabel(feature_names[i])
        plt.ylabel("Dependent Variable (y)")
        plt.title(f"{feature_names[i]} vs. y")
        plt.grid()

        if model_func is not None:
            x_sorted = np.sort(x[:, i])
            x_fixed = np.tile(np.mean(x, axis=0), (len(x_sorted), 1))
            x_fixed[:, i] = x_sorted
            y_pred = model_func(x_fixed)
            plt.plot(x_sorted, y_pred, color="red", label="Model Prediction")

        plt.legend()

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    print("Select a dataset option:")
    print("1: Quadratic + Sine with 3 features")
    print("2: Linear + Cosine with 5 features")
    print("3: Combination of Log, Sqrt, and Polynomial with 8 features")
    print("4: Sine, Cosine, and Cubic with 6 features")
    print("5: Quadratic + Exponential with 4 features")

    option = int(input("Enter the dataset option (1-5): ").strip())

    x_full, y_full = generate_multivariate_datasets(option)
    x_train, x_test, y_train, y_test = train_test_split(x_full, y_full, test_size=0.2, random_state=42)

    # Apply the recursive regression model with adaptive sampling
    model_func, best_degree = recursive_regression_with_adaptive_sampling_multivariate(
        x_train, y_train
    )

    # Evaluate the model on the test set
    y_test_pred = model_func(x_test)
    mse = mean_squared_error(y_test, y_test_pred)

    # Print test set MSE and best polynomial degree
    print(f"Test Set Mean Squared Error (MSE): {mse:.4f}")
    print(f"Best polynomial degree: {best_degree}")

    # Plot test set predictions
    plt.figure(figsize=(12, 6))
    plt.scatter(y_test, y_test_pred, alpha=0.7, label="Predicted vs Actual")
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], "r--", label="Ideal Fit")
    plt.xlabel("Actual Values")
    plt.ylabel("Predicted Values")
    plt.title("Test Set Predictions")
    plt.legend()
    plt.grid()
    plt.show()

    # Plot residuals for the test set
    residuals_test = y_test - y_test_pred
    plt.figure(figsize=(8, 6))
    plt.scatter(y_test, residuals_test, alpha=0.7, label="Residuals")
    plt.axhline(0, color="r", linestyle="--", label="Zero Residual")
    plt.xlabel("Actual Values")
    plt.ylabel("Residuals")
    plt.title("Test Set Residuals")
    plt.legend()
    plt.grid()
    plt.show()

    # Plot features and their relationship with the dependent variable
    plot_features_and_model(x_train, y_train, model_func)
